{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "!pip install pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"pyspark-assessment\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark {spark.version} initialized\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder. \\\n",
    "    appName(\"pyspark-1\"). \\df_raw = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"quote\", \"\\\"\")\n",
    "    .option(\"escape\", \"\\\"\")\n",
    "    .option(\"mode\", \"PERMISSIVE\")\n",
    "    .option(\"ignoreLeadingWhiteSpace\", \"true\")\n",
    "    .option(\"ignoreTrailingWhiteSpace\", \"true\")\n",
    "    .load(\"/Volumes/bms/kg/sample/nyc-jobs.csv\")\n",
    ")\n",
    "\n",
    "display(df_raw)\n",
    "\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, to_date\n",
    "\n",
    "df_converted = (\n",
    "    df_raw\n",
    "    # Posting Date\n",
    "    .withColumn(\n",
    "        \"posting_timestamp\",\n",
    "        to_timestamp(col(\"Posting Date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"posting_date\",\n",
    "        to_date(col(\"posting_timestamp\"))\n",
    "    )\n",
    "\n",
    "    # Posting Updated\n",
    "    .withColumn(\n",
    "        \"posting_updated_timestamp\",\n",
    "        to_timestamp(col(\"Posting Updated\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"posting_updated_date\",\n",
    "        to_date(col(\"posting_updated_timestamp\"))\n",
    "    )\n",
    "\n",
    "    # Process Date\n",
    "    .withColumn(\n",
    "        \"process_timestamp\",\n",
    "        to_timestamp(col(\"Process Date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"process_date\",\n",
    "        to_date(col(\"process_timestamp\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "df_converted.select(\n",
    "    \"Posting Date\",\n",
    "    \"posting_timestamp\",\n",
    "    \"posting_date\",\n",
    "    \"Posting Updated\",\n",
    "    \"posting_updated_timestamp\",\n",
    "    \"posting_updated_date\",\n",
    "    \"Process Date\",\n",
    "    \"process_timestamp\",\n",
    "    \"process_date\"\n",
    ").display(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import (\n",
    "    IntegerType, LongType, DoubleType, FloatType, StringType\n",
    ")\n",
    "\n",
    "def profile_data(df):\n",
    "    \"\"\"Generate comprehensive data profile\"\"\"\n",
    "\n",
    "    # Data types classification\n",
    "    numerical_cols = [\n",
    "        f.name for f in df.schema.fields\n",
    "        if isinstance(f.dataType, (IntegerType, LongType, DoubleType, FloatType))\n",
    "    ]\n",
    "    string_cols = [\n",
    "        f.name for f in df.schema.fields\n",
    "        if isinstance(f.dataType, StringType)\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nNumerical columns: {len(numerical_cols)}\")\n",
    "    print(f\"String columns: {len(string_cols)}\")\n",
    "\n",
    "    # Null analysis\n",
    "    print(f\"\\n{'Column':<35} {'Type':<15} {'Nulls':<10} {'Null %':<8}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    total = df.count()\n",
    "\n",
    "    for field in df.schema.fields:\n",
    "        col_name = field.name\n",
    "        dtype = field.dataType.simpleString()\n",
    "\n",
    "        if isinstance(field.dataType, StringType):\n",
    "            nulls = df.filter(\n",
    "                col(col_name).isNull() | (col(col_name) == '')\n",
    "            ).count()\n",
    "        else:\n",
    "            nulls = df.filter(col(col_name).isNull()).count()\n",
    "\n",
    "        null_pct = (nulls / total) * 100\n",
    "        print(f\"{col_name:<35} {dtype:<15} {nulls:<10} {null_pct:>6.1f}%\")\n",
    "\n",
    "    # Categorical cardinality\n",
    "    print(f\"\\n{'Categorical Column':<35} {'Unique Values'}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    for col_name in ['Job Category', 'Agency', 'Posting Type', 'Level', 'Salary Frequency']:\n",
    "        if col_name in df.columns:\n",
    "            unique = df.select(col_name).distinct().count()\n",
    "            print(f\"{col_name:<35} {unique:>10,}\")\n",
    "\n",
    "    return {\n",
    "        'numerical_cols': numerical_cols,\n",
    "        'string_cols': string_cols,\n",
    "        'total_rows': total\n",
    "    }\n",
    "profile_data(df_converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, regexp_replace, trim, initcap,\n",
    "    year, month\n",
    ")\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Final cleaning step after date conversion\n",
    "    Assumes posting_date, posting_updated_date, process_date already exist\n",
    "    \"\"\"\n",
    "\n",
    "    # Salary cleaning\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\n",
    "            \"salary_from\",\n",
    "            regexp_replace(col(\"Salary Range From\"), r\"[^\\d.]\", \"\").cast(\"double\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"salary_to\",\n",
    "            regexp_replace(col(\"Salary Range To\"), r\"[^\\d.]\", \"\").cast(\"double\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"salary_avg\",\n",
    "            (col(\"salary_from\") + col(\"salary_to\")) / 2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Positions\n",
    "    df = df.withColumn(\n",
    "        \"num_positions\",\n",
    "        regexp_replace(col(\"# Of Positions\"), r\"\\D\", \"\").cast(\"int\")\n",
    "    )\n",
    "\n",
    "    #  Date dimensions (SAFE â€“ no parsing)\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"posting_year\", year(col(\"posting_date\")))\n",
    "        .withColumn(\"posting_month\", month(col(\"posting_date\")))\n",
    "    )\n",
    "\n",
    "    # Text standardization\n",
    "    for c in [\"Agency\", \"Job Category\", \"Business Title\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(c, trim(initcap(col(c))))\n",
    "\n",
    "    #  Deduplication\n",
    "    df = df.dropDuplicates([\"Job ID\"])\n",
    "\n",
    "    #  Null handling (only where safe)\n",
    "    df = df.fillna({\n",
    "        \"Agency\": \"Unknown\",\n",
    "        \"Job Category\": \"Unknown\",\n",
    "        \"salary_avg\": 0,\n",
    "        \"num_positions\": 1\n",
    "    })\n",
    "\n",
    "    return df\n",
    "df_clean = clean_data(df_converted)\n",
    "display(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Apply feature engineering techniques\"\"\"\n",
    "    \n",
    "    # 1. Salary binning\n",
    "    df = df.withColumn('salary_category',\n",
    "        when(col('salary_avg') < 40000, 'Entry')\n",
    "        .when(col('salary_avg') < 70000, 'Mid')\n",
    "        .when(col('salary_avg') < 100000, 'Senior')\n",
    "        .when(col('salary_avg') >= 100000, 'Executive')\n",
    "        .otherwise('Unknown'))\n",
    "    \n",
    "    # 2. Posting age\n",
    "    df = df.withColumn('posting_age_days', \n",
    "                       datediff(current_date(), col('posting_date'))) \\\n",
    "           .withColumn('posting_status',\n",
    "                       when(col('posting_age_days') <= 30, 'Recent')\n",
    "                       .when(col('posting_age_days') <= 90, 'Active')\n",
    "                       .otherwise('Old'))\n",
    "    \n",
    "    # 3. Text features\n",
    "    df = df.withColumn('description_length', length(col('Job Description'))) \\\n",
    "           .withColumn('has_preferred_skills', \n",
    "                       when(col('Preferred Skills').isNotNull(), 1).otherwise(0)) \\\n",
    "           .withColumn('is_fulltime',\n",
    "                       when(col('Full-Time/Part-Time indicator').like('%F%'), 1).otherwise(0))\n",
    "    \n",
    "    # 4. Education extraction\n",
    "    df = df.withColumn('requires_phd',\n",
    "                       when(lower(col('Minimum Qual Requirements')).like('%phd%'), 1).otherwise(0)) \\\n",
    "           .withColumn('requires_masters',\n",
    "                       when(lower(col('Minimum Qual Requirements')).like('%master%'), 1).otherwise(0)) \\\n",
    "           .withColumn('requires_bachelors',\n",
    "                       when(lower(col('Minimum Qual Requirements')).like('%bachelor%'), 1).otherwise(0)) \\\n",
    "           .withColumn('education_level',\n",
    "                       when(col('requires_phd') == 1, 'PhD')\n",
    "                       .when(col('requires_masters') == 1, 'Masters')\n",
    "                       .when(col('requires_bachelors') == 1, 'Bachelors')\n",
    "                       .otherwise('High School'))\n",
    "    \n",
    "    # 5. Aggregated features\n",
    "    agency_stats = df.groupBy('Agency').agg(avg('salary_avg').alias('agency_avg_salary'))\n",
    "    df = df.join(agency_stats, 'Agency', 'left') \\\n",
    "           .withColumn('salary_vs_agency_avg', col('salary_avg') - col('agency_avg_salary'))\n",
    "    \n",
    "    print(\"Features engineered\")\n",
    "    return df\n",
    "\n",
    "df_featured = engineer_features(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(df):\n",
    "    \"\"\"Remove high-null and redundant columns\"\"\"\n",
    "    \n",
    "    cols_to_drop = [\n",
    "        'Recruitment Contact', 'Residency Requirement', 'Additional Information',\n",
    "        'To Apply', 'Hours/Shift', 'Division/Work Unit', 'Work Location 1',\n",
    "        'Title Code No', 'Posting Updated', 'Process Date',\n",
    "        'Salary Range From', 'Salary Range To', '# Of Positions',\n",
    "        'Posting Date', 'Post Until'\n",
    "    ]\n",
    "    \n",
    "    df = df.drop(*[c for c in cols_to_drop if c in df.columns])\n",
    "    print(f\"Selected {len(df.columns)} features\")\n",
    "    return df\n",
    "\n",
    "df_final = select_features(df_featured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPI 1: Top 10 job categories\n",
    "def kpi1_job_postings_by_category(df):\n",
    "    result = df.groupBy('Job Category') \\\n",
    "               .count() \\\n",
    "               .orderBy(desc('count')) \\\n",
    "               .limit(10) \\\n",
    "               .toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(result['Job Category'], result['count'], color='steelblue')\n",
    "    plt.xlabel('Number of Postings')\n",
    "    plt.title('Top 10 Job Categories')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('kpi1_categories.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return result\n",
    "kpi1_job_postings_by_category(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPI 2: Salary distribution by category\n",
    "def kpi2_salary_distribution(df):\n",
    "    result = df.filter(col('salary_avg') > 0) \\\n",
    "               .groupBy('Job Category') \\\n",
    "               .agg(\n",
    "                   avg('salary_avg').alias('avg_salary'),\n",
    "                   min('salary_avg').alias('min_salary'),\n",
    "                   max('salary_avg').alias('max_salary')\n",
    "               ) \\\n",
    "               .orderBy(desc('avg_salary')) \\\n",
    "               .limit(10) \\\n",
    "               .toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(result['Job Category'], result['avg_salary'], color='coral')\n",
    "    plt.xlabel('Average Salary ($)')\n",
    "    plt.title('Salary Distribution by Category')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('kpi2_salary_dist.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return result\n",
    "kpi2_salary_distribution(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPI 3: Education vs Salary correlation\n",
    "def kpi3_education_salary_correlation(df):\n",
    "    result = df.filter(col('salary_avg') > 0) \\\n",
    "               .groupBy('education_level') \\\n",
    "               .agg(avg('salary_avg').alias('avg_salary')) \\\n",
    "               .orderBy(desc('avg_salary')) \\\n",
    "               .toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    order = ['PhD', 'Masters', 'Bachelors', 'High School']\n",
    "    result_sorted = result.set_index('education_level').reindex(order).reset_index()\n",
    "    \n",
    "    plt.bar(\n",
    "    result_sorted['education_level'],\n",
    "    result_sorted['avg_salary'])\n",
    "\n",
    "    plt.ylabel('Average Salary ($)')\n",
    "    plt.title('Education Level vs Salary')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('kpi3_education_salary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nCorrelation: Higher education - Higher salary \")\n",
    "    return result\n",
    "\n",
    "kpi3_education_salary_correlation(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPI 4: Highest salary per agency\n",
    "def kpi4_highest_salary_per_agency(df):\n",
    "    window = Window.partitionBy('Agency').orderBy(desc('salary_avg'))\n",
    "    \n",
    "    result = df.filter(col('salary_avg') > 0) \\\n",
    "               .withColumn('rank', row_number().over(window)) \\\n",
    "               .filter(col('rank') == 1) \\\n",
    "               .select('Agency', 'Business Title', 'salary_avg') \\\n",
    "               .orderBy(desc('salary_avg')) \\\n",
    "               .limit(10) \\\n",
    "               .toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(result['Agency'], result['salary_avg'], color='darkgreen')\n",
    "    plt.xlabel('Highest Salary ($)')\n",
    "    plt.title('Top Paying Job per Agency')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('kpi4_highest_per_agency.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return result\n",
    "kpi4_highest_salary_per_agency(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, desc, max as spark_max\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def kpi5_avg_salary_last_2_years(df):\n",
    "\n",
    "    max_year = (\n",
    "        df.select(spark_max(col(\"posting_year\")).alias(\"max_year\"))\n",
    "          .collect()[0][\"max_year\"]\n",
    "    )\n",
    "\n",
    "    result = (\n",
    "        df.filter(\n",
    "            (col(\"posting_year\") >= max_year - 2) &\n",
    "            (col(\"salary_avg\") > 0)\n",
    "        )\n",
    "        .groupBy(\"Agency\")\n",
    "        .agg(avg(\"salary_avg\").alias(\"avg_salary\"))\n",
    "        .orderBy(desc(\"avg_salary\"))\n",
    "        .limit(10)\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(result[\"Agency\"], result[\"avg_salary\"])\n",
    "    plt.xlabel(\"Average Salary ($)\")\n",
    "    plt.title(f\"Average Salary per Agency (Last 2 Years in Data: {max_year-2}-{max_year})\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "kpi5_avg_salary_last_2_years(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kpi6_highest_paid_skills(df):\n",
    "    skills = ['python', 'java', 'sql', 'aws', 'azure', 'machine learning',\n",
    "              'data science', 'kubernetes', 'docker', 'react', 'leadership',\n",
    "              'project management', 'analytics', 'cybersecurity']\n",
    "    \n",
    "    skill_data = []\n",
    "\n",
    "    df_filtered = df.filter(col('salary_avg') > 0)\n",
    "    \n",
    "    for skill in skills:\n",
    "        skill_df = df_filtered.filter(\n",
    "            lower(col('Preferred Skills')).like(f'%{skill}%') |\n",
    "            lower(col('Minimum Qual Requirements')).like(f'%{skill}%') |\n",
    "            lower(col('Job Description')).like(f'%{skill}%')\n",
    "        )\n",
    "        \n",
    "        stats = skill_df.agg(avg('salary_avg').alias('avg_salary')).collect()\n",
    "        count = skill_df.count()\n",
    "        \n",
    "        if count > 0:\n",
    "            skill_data.append({\n",
    "                'Skill': skill.title(),\n",
    "                'avg_salary': stats[0]['avg_salary'],\n",
    "                'count': count\n",
    "            })\n",
    "    \n",
    "    result = pd.DataFrame(skill_data).sort_values('avg_salary', ascending=False).head(10)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    axes[0].barh(result['Skill'], result['avg_salary'])\n",
    "    axes[0].set_xlabel('Average Salary ($)')\n",
    "    axes[0].set_title('Highest Paid Skills')\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    axes[1].barh(result['Skill'], result['count'])\n",
    "    axes[1].set_xlabel('Job Count')\n",
    "    axes[1].set_title('Skill Demand')\n",
    "    axes[1].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('kpi6_skills.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return result\n",
    "kpi6_highest_paid_skills(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute KPIs\n",
    "print(\"KPI ANALYSIS\")\n",
    "\n",
    "kpi1 = kpi1_job_postings_by_category(df_final)\n",
    "kpi2 = kpi2_salary_distribution(df_final)\n",
    "kpi3 = kpi3_education_salary_correlation(df_final)\n",
    "kpi4 = kpi4_highest_salary_per_agency(df_final)\n",
    "kpi5 = kpi5_avg_salary_last_2_years(df_final)\n",
    "kpi6 = kpi6_highest_paid_skills(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_outputs(df):\n",
    "    \"\"\"Save processed data and KPI results\"\"\"\n",
    "    \n",
    "    # Save processed data\n",
    "    df.write.mode('overwrite').parquet('/Volumes/bms/kg/sample/nyc_jobs_processed.parquet')\n",
    "    \n",
    "    print(f\"Saved {df.count():,} processed records\")\n",
    "\n",
    "save_outputs(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    \"\"\"Execute validation tests\"\"\"\n",
    "    \n",
    "    tests_passed = 0\n",
    "    tests_total = 8\n",
    "    \n",
    "    # Test 1: Data loaded\n",
    "    try:\n",
    "        assert df_raw.count() > 0\n",
    "        print(\" Test 1: Data loaded\")\n",
    "        tests_passed += 1\n",
    "    except:\n",
    "        print(\" Test 1: Failed\")\n",
    "    \n",
    "    # Test 2: Salary cleaning\n",
    "    try:\n",
    "        assert 'salary_avg' in df_clean.columns\n",
    "        assert df_clean.select('salary_avg').dtypes[0][1] == 'double'\n",
    "        print(\" Test 2: Salary cleaned\")\n",
    "        tests_passed += 1\n",
    "    except:\n",
    "        print(\" Test 2: Failed\")\n",
    "    \n",
    "    # Test 3: Features engineered\n",
    "    try:\n",
    "        assert all(c in df_featured.columns for c in ['salary_category', 'education_level'])\n",
    "        print(\" Test 3: Features engineered\")\n",
    "        tests_passed += 1\n",
    "    except:\n",
    "        print(\" Test 3: Failed\")\n",
    "    \n",
    "    # Test 4: No nulls in critical columns\n",
    "    try:\n",
    "        assert df_final.filter(col('Job Category').isNull()).count() == 0\n",
    "        print(\" Test 4: No nulls in critical columns\")\n",
    "        tests_passed += 1\n",
    "    except:\n",
    "        print(\" Test 4: Failed\")\n",
    "    \n",
    "    # Test 5: No duplicates\n",
    "    try:\n",
    "        assert df_final.count() == df_final.select('Job ID').distinct().count()\n",
    "        print(\" Test 5: No duplicates\")\n",
    "        tests_passed += 1\n",
    "    except:\n",
    "        print(\" Test 5: Failed\")\n",
    "    \n",
    "    # Test 6: Salary ranges valid\n",
    "    try:\n",
    "        invalid = df_final.filter(\n",
    "            (col('salary_to') < col('salary_from')) & (col('salary_from') > 0)\n",
    "        ).count()\n",
    "        assert invalid == 0\n",
    "        print(\" Test 6: Salary ranges valid\")\n",
    "        tests_passed += 1\n",
    "    except:\n",
    "        print(\" Test 6: Failed\")\n",
    "    \n",
    "    # Test 7: Dates parsed\n",
    "    try:\n",
    "        assert 'posting_date' in df_final.columns\n",
    "        print(\" Test 7: Dates parsed\")\n",
    "        tests_passed += 1\n",
    "    except:\n",
    "        print(\" Test 7: Failed\")\n",
    "    \n",
    "\n",
    "run_tests()\n",
    "\n",
    "# SUMMARY\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(f\"Original records: {profile['total_rows']:,}\")\n",
    "print(f\"Final records: {df_final.count():,}\")\n",
    "print(f\"Features: {len(df_final.columns)}\")\n",
    "print(f\"KPIs analyzed: 6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
